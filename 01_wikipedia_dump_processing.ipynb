{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import codecs\n",
    "import re\n",
    "import csv \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_article_text(article_txt):\n",
    "    # remove text written between double curly braces\n",
    "    article_txt = re.sub(r\"{{.*}}\",\"\",article_txt)\n",
    "\n",
    "    # remove file attachments\n",
    "    article_txt = re.sub(r\"\\[\\[File:.*\\]\\]\",\"\",article_txt)\n",
    "\n",
    "    # remove Image attachments\n",
    "    article_txt = re.sub(r\"\\[\\[Image:.*\\]\\]\",\"\",article_txt)\n",
    "\n",
    "    # remove unwanted lines starting from special characters\n",
    "    article_txt = re.sub(r\"\\n: \\'\\'.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"\\n!.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"^:\\'\\'.*\",\"\",article_txt)\n",
    "\n",
    "    # remove non-breaking space symbols\n",
    "    article_txt = re.sub(r\"&nbsp\",\"\",article_txt)\n",
    "\n",
    "    # remove URLs link\n",
    "    article_txt = re.sub(r\"http\\S+\",\"\",article_txt)\n",
    "\n",
    "    # remove digits from text\n",
    "    article_txt = re.sub(r\"\\d+\",\"\",article_txt)\n",
    "\n",
    "    # remove text written between small braces   \n",
    "    article_txt = re.sub(r\"\\(.*\\)\",\"\",article_txt)\n",
    "\n",
    "    # remove sentence which tells category of article\n",
    "    article_txt = re.sub(r\"Category:.*\",\"\",article_txt)\n",
    "\n",
    "    # remove the sentences inside infobox or taxobox\n",
    "    article_txt = re.sub(r\"\\| .*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"\\n\\|.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"\\n \\|.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\".* \\|\\n\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\".*\\|\\n\",\"\",article_txt)\n",
    "\n",
    "    # remove infobox or taxobox\n",
    "    article_txt = re.sub(r\"{{Infobox.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"{{infobox.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"{{taxobox.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"{{Taxobox.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"{{ Infobox.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"{{ infobox.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"{{ taxobox.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"{{ Taxobox.*\",\"\",article_txt)\n",
    "\n",
    "    # remove lines starting from *\n",
    "    article_txt = re.sub(r\"\\* .*\",\"\",article_txt)\n",
    "\n",
    "    # remove text written between angle bracket\n",
    "    article_txt = re.sub(\"[\\<].*?[\\>]\", \"\", article_txt)\n",
    "\n",
    "    # Remove all headings\n",
    "    article_txt = re.sub(\"[\\==].*?[\\==]\", \"\", article_txt)\n",
    "    \n",
    "    # remove new line character\n",
    "    article_txt = re.sub(r\"\\n\",\"\",article_txt)  \n",
    "\n",
    "    # replace all punctuations with space \n",
    "    article_txt = re.sub(r\"\\!|\\\"|\\#|\\$|\\%|\\&|\\'|\\(|\\)|\\*|\\+|\\,|\\-|\\.|\\/|\\:|\\;|\\|\\?|\\@|\\[|\\\\|\\]|\\^|\\_|\\`|\\{|\\||\\}|\\~\",\" \",article_txt)\n",
    "\n",
    "    # replace consecutive multiple space with single space\n",
    "    article_txt = re.sub(r\" +\",\" \",article_txt)\n",
    "\n",
    "    # replace non-breaking space with regular space \n",
    "    article_txt = article_txt.replace(u'\\xa0', u' ')\n",
    "    \n",
    "    return article_txt\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse('data/wikipedia/simplewiki-20180901-pages-meta-current.xml')  \n",
    "root = tree.getroot()  \n",
    "path = 'articles-corpus//' \n",
    "url  = '{http://www.mediawiki.org/xml/export-0.10/}page'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_name = 'data/wikipedia/processed_data.csv'\n",
    "headers = ['wiki_title', 'text']\n",
    "\n",
    "def append_row(csv_file_path, row):\n",
    "    \"\"\"This function was written to deal with having files left open\"\"\"\n",
    "    with open(csv_file_path, encoding='UTF-16', mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)\n",
    "\n",
    "# intialize file\n",
    "with open(csv_file_name, 'w', encoding='UTF-16') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,page in enumerate(root.findall(url)):\n",
    "    \n",
    "    for p in page:\n",
    "        if p.tag == '{http://www.mediawiki.org/xml/export-0.10/}title':\n",
    "            title = p.text\n",
    "        r_tag = \"{http://www.mediawiki.org/xml/export-0.10/}revision\"                 \n",
    "        if p.tag == r_tag:  \n",
    "            for x in p:\n",
    "#                 print(x.tag)\n",
    "                tag = \"{http://www.mediawiki.org/xml/export-0.10/}text\"\n",
    "                \n",
    "                if x.tag == tag:                                                              \n",
    "                    text = x.text                                          \n",
    "                    if not text == None:  \n",
    "                        # Extracting the text portion from the article                                                 \n",
    "#                         text = text[:text.find(\"==\")]\n",
    "#                         print(title)\n",
    "#                         print(text)\n",
    "                        # <em><strong>Cleaning of Text (described in Section 2)</strong></em>                                                     \n",
    "                        # Printing the article \n",
    "#                         print(title)\n",
    "#                         print(text)\n",
    "#                         print('\\n====================================\\n')\n",
    "                        if 'User:' in title:\n",
    "                            continue\n",
    "                        elif 'Talk:' in title:\n",
    "                            continue\n",
    "                        elif 'Wikipedia' in title:\n",
    "                            continue\n",
    "                        elif 'talk:' in title:\n",
    "                            continue\n",
    "                        elif 'Category:' in title:\n",
    "                            continue\n",
    "                        \n",
    "                        text = clean_article_text(text)\n",
    "                        append_row(csv_file_name, [title, text])\n",
    "#                         input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv('data/wikipedia/processed_data.csv', encoding='UTF-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = data_frame[-data_frame.text.apply(lambda x: 'REDIRECT' in str(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = data_frame[-data_frame.wiki_title.apply(lambda x: 'Template:' in str(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = data_frame[data_frame.text.apply(lambda x: len(str(x))>200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total articles: 98939\n"
     ]
    }
   ],
   "source": [
    "print('Total articles: {}'.format(len(data_frame)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's subset these articles to train on. Let's consider 50,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed('computer what?????')\n",
    "\n",
    "wikipedia_subset = data_frame.sample(n=70000, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.to_csv('data/wikipedia/cleaned_wiki_data_full_text.csv')\n",
    "wikipedia_subset.to_csv('data/wikipedia/subsetted_wiki_data_full_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
