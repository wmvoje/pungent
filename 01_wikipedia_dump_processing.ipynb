{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import codecs\n",
    "import re\n",
    "import csv \n",
    "import wikidata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_article_text(article_txt):\n",
    "    # remove text written between double curly braces\n",
    "    article_txt = re.sub(r\"{{.*}}\",\"\",article_txt)\n",
    "\n",
    "    # remove file attachments\n",
    "    article_txt = re.sub(r\"\\[\\[File:.*\\]\\]\",\"\",article_txt)\n",
    "\n",
    "    # remove Image attachments\n",
    "    article_txt = re.sub(r\"\\[\\[Image:.*\\]\\]\",\"\",article_txt)\n",
    "\n",
    "    # remove unwanted lines starting from special characters\n",
    "    article_txt = re.sub(r\"\\n: \\'\\'.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"\\n!.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"^:\\'\\'.*\",\"\",article_txt)\n",
    "\n",
    "    # remove non-breaking space symbols\n",
    "    article_txt = re.sub(r\"&nbsp\",\"\",article_txt)\n",
    "\n",
    "    # remove URLs link\n",
    "    article_txt = re.sub(r\"http\\S+\",\"\",article_txt)\n",
    "\n",
    "    # remove digits from text\n",
    "    article_txt = re.sub(r\"\\d+\",\"\",article_txt)\n",
    "\n",
    "    # remove text written between small braces   \n",
    "    article_txt = re.sub(r\"\\(.*\\)\",\"\",article_txt)\n",
    "\n",
    "    # remove sentence which tells category of article\n",
    "    article_txt = re.sub(r\"Category:.*\",\"\",article_txt)\n",
    "\n",
    "    # remove the sentences inside infobox or taxobox\n",
    "    article_txt = re.sub(r\"\\| .*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"\\n\\|.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"\\n \\|.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\".* \\|\\n\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\".*\\|\\n\",\"\",article_txt)\n",
    "\n",
    "    # remove infobox or taxobox\n",
    "    article_txt = re.sub(r\"{{Infobox.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"{{infobox.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"{{taxobox.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"{{Taxobox.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"{{ Infobox.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"{{ infobox.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"{{ taxobox.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"{{ Taxobox.*\",\"\",article_txt)\n",
    "\n",
    "    # remove lines starting from *\n",
    "    article_txt = re.sub(r\"\\* .*\",\"\",article_txt)\n",
    "\n",
    "    # remove text written between angle bracket\n",
    "    article_txt = re.sub(\"[\\<].*?[\\>]\", \"\", article_txt)\n",
    "    \n",
    "    # remove new line character\n",
    "    article_txt = re.sub(r\"\\n\",\"\",article_txt)  \n",
    "\n",
    "    # replace all punctuations with space \n",
    "    article_txt = re.sub(r\"\\!|\\\"|\\#|\\$|\\%|\\&|\\'|\\(|\\)|\\*|\\+|\\,|\\-|\\.|\\/|\\:|\\;|\\|\\?|\\@|\\[|\\\\|\\]|\\^|\\_|\\`|\\{|\\||\\}|\\~\",\" \",article_txt)\n",
    "\n",
    "    # replace consecutive multiple space with single space\n",
    "    article_txt = re.sub(r\" +\",\" \",article_txt)\n",
    "\n",
    "    # replace non-breaking space with regular space \n",
    "    article_txt = article_txt.replace(u'\\xa0', u' ')\n",
    "    \n",
    "    return article_txt\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_headings(text):\n",
    "    output = []\n",
    "    for i in re.split(\"[\\==].*?[\\==]\", clean_article_text(text)):\n",
    "        if len(i) > 150:\n",
    "            # Filter out short enteries (likely headings themselves)\n",
    "            output.append(i)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse('data/wikipedia/simplewiki-20180901-pages-meta-current.xml')  \n",
    "root = tree.getroot()  \n",
    "path = 'articles-corpus//' \n",
    "url  = '{http://www.mediawiki.org/xml/export-0.10/}page'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_name = 'data/wikipedia/processed_data_split.csv'\n",
    "headers = ['wiki_title', 'text']\n",
    "\n",
    "def append_row(csv_file_path, row):\n",
    "    \"\"\"This function was written to deal with having files left open\"\"\"\n",
    "    with open(csv_file_path, encoding='UTF-16', mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)\n",
    "\n",
    "# intialize file\n",
    "with open(csv_file_name, 'w', encoding='UTF-16') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,page in enumerate(root.findall(url)):\n",
    "    \n",
    "    for p in page:\n",
    "\n",
    "        if p.tag == '{http://www.mediawiki.org/xml/export-0.10/}title':\n",
    "            title = p.text\n",
    " \n",
    "        r_tag = \"{http://www.mediawiki.org/xml/export-0.10/}revision\"                 \n",
    "        if p.tag == r_tag:  \n",
    "            \n",
    "            for x in p:\n",
    "#                 print(x.tag)\n",
    "                tag = \"{http://www.mediawiki.org/xml/export-0.10/}text\"\n",
    "                \n",
    "                if x.tag == tag:                                                              \n",
    "                    text = x.text                                          \n",
    "                    if not text == None:  \n",
    "                        # Extracting the text portion from the article                                                 \n",
    "#                         text = text[:text.find(\"==\")]\n",
    "#                         print(title)\n",
    "#                         print(text)\n",
    "                        # <em><strong>Cleaning of Text (described in Section 2)</strong></em>                                                     \n",
    "                        # Printing the article \n",
    "#                         print(title)\n",
    "#                         print(text)\n",
    "#                         print('\\n====================================\\n')\n",
    "                        if 'User:' in title:\n",
    "                            continue\n",
    "                        elif 'Talk:' in title:\n",
    "                            continue\n",
    "                        elif 'Wikipedia' in title:\n",
    "                            continue\n",
    "                        elif 'talk:' in title:\n",
    "                            continue\n",
    "                        elif 'Category:' in title:\n",
    "                            continue\n",
    "\n",
    "                        text = clean_article_text(text)\n",
    "                        text_chunks = split_text_into_headings(text)\n",
    "                        for count, textbit in enumerate(text_chunks):\n",
    "                            \n",
    "                            append_row(csv_file_name, [title+'--'+str(count), textbit])\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_headings(text):\n",
    "    output = []\n",
    "    for i in re.split(\"[\\==].*?[\\==]\", clean_article_text(text)):\n",
    "        if len(i) > 150:\n",
    "            # Filter out short enteries (likely headings themselves)\n",
    "            output.append(i)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv('data/wikipedia/processed_data_split.csv', encoding='UTF-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = data_frame[-data_frame.wiki_title.apply(lambda x: 'Template:' in str(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = data_frame[-data_frame.text.apply(lambda x: 'REDIRECT' in str(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = data_frame[data_frame.text.apply(lambda x: len(str(x))>200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = data_frame[-data_frame.wiki_title.apply(lambda x: 'Module:' in str(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total articles: 152895\n"
     ]
    }
   ],
   "source": [
    "print('Total articles: {}'.format(len(data_frame)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's subset these articles to train on. Let's consider 50,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed('computer what?????')\n",
    "\n",
    "wikipedia_subset = data_frame.sample(n=120000, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.to_csv('data/wikipedia/cleaned_wiki_data_full_text_chunks.csv')\n",
    "wikipedia_subset.to_csv('data/wikipedia/subsetted_wiki_data_full_text_text_chunks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'[-1 -1 -1 ...  0 -1 -1] not in index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7ff5b7110813>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_frame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwikipedia_subset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2677\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2678\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2679\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2680\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2721\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2722\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2723\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2724\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1325\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[0;32m-> 1327\u001b[0;31m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '[-1 -1 -1 ...  0 -1 -1] not in index'"
     ]
    }
   ],
   "source": [
    "data_frame[data_frame.index.isin(wikipedia_subset.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data_frame[~data_frame.index.isin(wikipedia_subset.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('data/wikipedia/subsetted_wiki_data_full_text_text_chunks_leftout.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ..., False,  True,  True])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
