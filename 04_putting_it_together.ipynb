{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal \n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "1. Take an input context text \n",
    "2. Take input sentence\n",
    "\n",
    "**Backend:**\n",
    "\n",
    "1. Convert input(1) to tokens \n",
    "2. wikpedia_topic_mode(tokens) -> likely_topics\n",
    "3. wikpedia_topic_mode(likely_topics) -> pun_possible_words\n",
    "4. pun.insert_pun( input(2), pun_possible_words ) -> new sentence\n",
    "\n",
    "**Output:**\n",
    "\n",
    "1. Print new sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import pun\n",
    "import pickle\n",
    "from gensim.models.ldamodel import LdaModel as Lda\n",
    "from gensim import corpora\n",
    "from gensim.models import doc2vec\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "\n",
    "wiki_topicmodel = Lda.load('models/180925_wikipedia_model.individually_binned.200.gensim.')\n",
    "\n",
    "# loading the stemmed_dict\n",
    "\n",
    "with open('180922_stemmed_dict.p', 'rb') as tounpick:\n",
    "    stemmed_dict = pickle.load(tounpick)\n",
    "\n",
    "# Loading the doc2vec\n",
    "\n",
    "wiki_doc2vec = doc2vec.Doc2Vec.load('models/simple_wiki_chunked_doc2vec_300_vector_10_min_word')\n",
    "\n",
    "# loading the doc2vec corpus \n",
    "\n",
    "with open('models/simple_wiki_chunked_corpus_10_count_cutoff.p', 'rb') as tounpcik:\n",
    "    wiki_doc2vec_corpus = pickle.load(tounpcik)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with words from topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_from_top_topics(topic_list, model, min_topic_prob=0.1, min_word_prob=0.05):\n",
    "    \"\"\"\n",
    "    First finding all of the words\n",
    "    \"\"\"\n",
    "    \n",
    "    list_of_words = []\n",
    "    \n",
    "    topic_list.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    \n",
    "    for topic, topic_prob in topic_list:\n",
    "        \n",
    "        if topic_prob < min_topic_prob:\n",
    "            break\n",
    "        \n",
    "        for word_id, word_prob in model.get_topic_terms(topic, 100):\n",
    "            if word_prob < min_word_prob:\n",
    "                break\n",
    "            list_of_words.append(model.id2word[word_id])\n",
    "    \n",
    "    return list_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_from_top_topic(topic_list, model, min_word_prob=0.05):\n",
    "    \"\"\"\n",
    "    First finding all of the words\n",
    "    \"\"\"\n",
    "    \n",
    "    list_of_words = []\n",
    "    topic_list.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    \n",
    "    topic, topic_prob = topic_list[0]\n",
    "        \n",
    "    for word_id, word_prob in model.get_topic_terms(topic, 100):\n",
    "        if word_prob < min_word_prob:\n",
    "            break\n",
    "        if model.id2word[word_id] in stemmed_dict:\n",
    "            for word in stemmed_dict[model.id2word[word_id]]:\n",
    "                list_of_words.append(word)\n",
    "        else:\n",
    "            list_of_words.append(model.id2word[word_id])\n",
    "\n",
    "    \n",
    "    return list_of_words, topic_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the context\n",
    "\n",
    "context = 'purr'\n",
    "context = \"Climbing is the activity of using one's hands, feet, or any other part of the body to ascend a steep object. It is done for locomotion, recreation and competition, in trades that rely on it, and in emergency rescue and military operations. It is done indoors and out, on natural and man-made structures.\"\n",
    "# context = \"The domestic cat (Felis silvestris catus or Felis catus)[1][4] is a small, typically furry, carnivorous mammal. They are often called house cats[5] when kept as indoor pets or simply cats when there is no need to distinguish them from other felids and felines. They are often valued by humans for companionship and for their ability to hunt vermin. There are more than seventy cat breeds recognized by various cat registries.\"\n",
    "\n",
    "# Tokenize context \n",
    "\n",
    "context_tokens = pun.tokenize(context)\n",
    "\n",
    "# Convert to bow\n",
    "\n",
    "bag_of_words = wiki_topicmodel.id2word.doc2bow(context_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_topicmodel_words(sentence, model):\n",
    "    context_tokens = pun.tokenize(sentence)\n",
    "    bag_of_words = model.id2word.doc2bow(context_tokens)\n",
    "    document_topics = model.get_document_topics(bag_of_words) \n",
    "    return get_words_from_top_topic(document_topics, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['wear', 'feet', 'walk'], 0.12638511)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_to_topicmodel_words(context, wiki_topicmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with doc2vec information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_doc2vec(text, model):\n",
    "    \"\"\"\n",
    "    Iterator which spits out words that are found in \n",
    "    the most 'topical' textual elements\n",
    "    \"\"\"\n",
    "    # parse the sentence \n",
    "    text = simple_preprocess(text)\n",
    "    # Find the respective doc2vec vector\n",
    "    text_vector = model.infer_vector(text)\n",
    "    # find the most similar text pieces\n",
    "    most_similar_documents_with_score = model.docvecs.most_similar([text_vector])\n",
    "    \n",
    "    for document_id, cosine_sim_score in most_similar_documents_with_score:\n",
    "        \n",
    "        yield (set(pun.tokenize(wiki_doc2vec_corpus[document_id], stem=False, initial_word_split=False)), cosine_sim_score)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = sentence_to_doc2vec(\"Climbing is the activity of using one's hands, feet, or any other part of the body to ascend a steep object. It is done for locomotion, recreation and competition, in trades that rely on it, and in emergency rescue and military operations. It is done indoors and out, on natural and man-made structures.\",\n",
    "                             wiki_doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object sentence_to_doc2vec at 0x7f62a6ff61a8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wmvoje/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'examples',\n",
       "  'harm',\n",
       "  'include',\n",
       "  'inhibition',\n",
       "  'organism',\n",
       "  'plant',\n",
       "  'relationship',\n",
       "  'secretions',\n",
       "  'shade',\n",
       "  'taller',\n",
       "  'wider'},\n",
       " 0.36731237173080444)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Builiding how it is all connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_possible_pun_substitutions(context, input_sentence, w2v_number=4):\n",
    "    \"\"\"\n",
    "    Takes context and input sentence \n",
    "    \n",
    "    returns list of possible substitutions with scores and the topic \n",
    "    words consdiered\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # First process context\n",
    "    doc2vec_word_generator = sentence_to_doc2vec(context, wiki_doc2vec)\n",
    "    topic_words, topic_score = sentence_to_topicmodel_words(context, wiki_topicmodel)\n",
    "\n",
    "    # Then try to generate sentences using these metrics\n",
    "    output = []\n",
    "    topic_words_considered = []\n",
    "    \n",
    "    # consider word2vec words\n",
    "    for i in range(w2v_number):\n",
    "        words, w2v_score = next(doc2vec_word_generator)\n",
    "\n",
    "        topic_words_considered.extend([[word, 'doc2vec', i+1, w2v_score] for word in words])\n",
    "\n",
    "        sub_tuples = pun.enumerate_PD_pun_subs(input_sentence, words)\n",
    "        # word, sub_index, phonetic_distance\n",
    "\n",
    "        for word, sub_index, phon_dist in sub_tuples:\n",
    "            output.append([word, sub_index, phon_dist, 'doc2vec', i+1, w2v_score, phon_dist/w2v_score])\n",
    "\n",
    "    # Now do topic words\n",
    "    sub_tuples = pun.enumerate_PD_pun_subs(input_sentence, topic_words)\n",
    "    for word, sub_index, phon_dist in sub_tuples:\n",
    "\n",
    "        topic_words_considered.append([word, 'topicModel', 1, topic_score])\n",
    "\n",
    "        output.append([word, sub_index, phon_dist, 'topicModel', 1, topic_score, phon_dist/topic_score])\n",
    "\n",
    "    output.sort(key=lambda x: x[6])\n",
    "    \n",
    "    return output, topic_words_considered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context = \"Climbing is the activity of using one's hands, feet, or any other part of the body to ascend a steep object. It is done for locomotion, recreation and competition, in trades that rely on it, and in emergency rescue and military operations. It is done indoors and out, on natural and man-made structures.\"\n",
    "context = \"Prince of the United Kingdom of Great Britain and Northern Ireland is a royal title normally granted to sons and grandsons of reigning and past British monarchs. It is also held by the Duke of Edinburgh, husband and consort of Queen Elizabeth II. The title is granted by the reigning monarch, who is the fount of all honours, through the issuing of letters patent as an expression of the royal will.\"\n",
    "\n",
    "input_sentence = \"join us at our gym we hope to see you there\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wmvoje/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "ranked_substitutions, topic_words_considered = generate_possible_pun_substitutions(context, input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join us at flower gym we hope to see you there\n",
      "join us at our gym we hope to see use there\n",
      "join us at our gym we hope to see you wear\n",
      "join us at our gym we hope to sir you there\n",
      "join us at our gym see hope to see you there\n",
      "join us at our gym way hope to see you there\n",
      "join use at our gym we hope to see you there\n",
      "join us at our gym queen hope to see you there\n",
      "join sir at our gym we hope to see you there\n",
      "join pass at our gym we hope to see you there\n"
     ]
    }
   ],
   "source": [
    "for thing in [pun.substitute_pun(input_sentence, x[:3]) for x in ranked_substitutions[:10]]:\n",
    "    print(thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['flower', 3, 2, 'doc2vec', 1, 0.7421815395355225, 2.6947584835533025],\n",
       " ['use', 9, 2, 'doc2vec', 1, 0.7421815395355225, 2.6947584835533025],\n",
       " ['wear', 10, 2, 'doc2vec', 1, 0.7421815395355225, 2.6947584835533025],\n",
       " ['sir', 8, 2, 'doc2vec', 2, 0.7250586152076721, 2.758397677168704],\n",
       " ['see', 5, 2, 'doc2vec', 3, 0.7234101295471191, 2.764683432414849],\n",
       " ['way', 5, 2, 'doc2vec', 3, 0.7234101295471191, 2.764683432414849],\n",
       " ['use', 1, 3, 'doc2vec', 1, 0.7421815395355225, 4.042137725329954],\n",
       " ['queen', 5, 3, 'doc2vec', 1, 0.7421815395355225, 4.042137725329954],\n",
       " ['sir', 1, 3, 'doc2vec', 2, 0.7250586152076721, 4.1375965157530565],\n",
       " ['pass', 1, 3, 'doc2vec', 2, 0.7250586152076721, 4.1375965157530565],\n",
       " ['man', 2, 3, 'doc2vec', 2, 0.7250586152076721, 4.1375965157530565],\n",
       " ['pass', 2, 3, 'doc2vec', 2, 0.7250586152076721, 4.1375965157530565],\n",
       " ['knight', 2, 3, 'doc2vec', 2, 0.7250586152076721, 4.1375965157530565],\n",
       " ['see', 1, 3, 'doc2vec', 3, 0.7234101295471191, 4.147025148622274],\n",
       " ['pass', 1, 3, 'doc2vec', 3, 0.7234101295471191, 4.147025148622274],\n",
       " ['base', 1, 3, 'doc2vec', 3, 0.7234101295471191, 4.147025148622274],\n",
       " ['pass', 2, 3, 'doc2vec', 3, 0.7234101295471191, 4.147025148622274],\n",
       " ['past', 2, 3, 'doc2vec', 3, 0.7234101295471191, 4.147025148622274],\n",
       " ['house', 1, 3, 'doc2vec', 4, 0.7207200527191162, 4.162503858025968],\n",
       " ['coat', 2, 3, 'doc2vec', 4, 0.7207200527191162, 4.162503858025968],\n",
       " ['house', 3, 3, 'doc2vec', 4, 0.7207200527191162, 4.162503858025968],\n",
       " ['chain', 0, 4, 'doc2vec', 1, 0.7421815395355225, 5.389516967106605],\n",
       " ['win', 0, 4, 'doc2vec', 1, 0.7421815395355225, 5.389516967106605],\n",
       " ['dance', 1, 4, 'doc2vec', 1, 0.7421815395355225, 5.389516967106605],\n",
       " ['part', 2, 4, 'doc2vec', 1, 0.7421815395355225, 5.389516967106605],\n",
       " ['crown', 3, 4, 'doc2vec', 1, 0.7421815395355225, 5.389516967106605],\n",
       " ['win', 4, 4, 'doc2vec', 1, 0.7421815395355225, 5.389516967106605],\n",
       " ['king', 4, 4, 'doc2vec', 1, 0.7421815395355225, 5.389516967106605],\n",
       " ['win', 5, 4, 'doc2vec', 1, 0.7421815395355225, 5.389516967106605],\n",
       " ['wear', 5, 4, 'doc2vec', 1, 0.7421815395355225, 5.389516967106605],\n",
       " ['head', 6, 4, 'doc2vec', 1, 0.7421815395355225, 5.389516967106605],\n",
       " ['type', 6, 4, 'doc2vec', 1, 0.7421815395355225, 5.389516967106605],\n",
       " ['use', 7, 4, 'doc2vec', 1, 0.7421815395355225, 5.389516967106605],\n",
       " ['choose', 7, 4, 'doc2vec', 1, 0.7421815395355225, 5.389516967106605],\n",
       " ['type', 7, 4, 'doc2vec', 1, 0.7421815395355225, 5.389516967106605],\n",
       " ['use', 8, 4, 'doc2vec', 1, 0.7421815395355225, 5.389516967106605],\n",
       " ['receive', 8, 4, 'doc2vec', 1, 0.7421815395355225, 5.389516967106605],\n",
       " ['choose', 9, 4, 'doc2vec', 1, 0.7421815395355225, 5.389516967106605],\n",
       " ['head', 10, 4, 'doc2vec', 1, 0.7421815395355225, 5.389516967106605],\n",
       " ['man', 0, 4, 'doc2vec', 2, 0.7250586152076721, 5.516795354337408],\n",
       " ['sir', 2, 4, 'doc2vec', 2, 0.7250586152076721, 5.516795354337408],\n",
       " ['sir', 3, 4, 'doc2vec', 2, 0.7250586152076721, 5.516795354337408],\n",
       " ['order', 3, 4, 'doc2vec', 2, 0.7250586152076721, 5.516795354337408],\n",
       " ['sir', 5, 4, 'doc2vec', 2, 0.7250586152076721, 5.516795354337408],\n",
       " ['sir', 7, 4, 'doc2vec', 2, 0.7250586152076721, 5.516795354337408],\n",
       " ['knight', 7, 4, 'doc2vec', 2, 0.7250586152076721, 5.516795354337408],\n",
       " ['pass', 8, 4, 'doc2vec', 2, 0.7250586152076721, 5.516795354337408],\n",
       " ['sir', 9, 4, 'doc2vec', 2, 0.7250586152076721, 5.516795354337408],\n",
       " ['know', 0, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['law', 1, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['know', 1, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['way', 1, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['die', 1, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['see', 2, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['law', 2, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['quit', 2, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['know', 2, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['way', 2, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['die', 2, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['see', 3, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['law', 3, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['crown', 3, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['know', 3, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['way', 3, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['die', 3, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['order', 3, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['time', 4, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['king', 4, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['law', 5, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['know', 5, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['die', 5, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['know', 6, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['see', 7, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['law', 7, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['time', 7, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['know', 7, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['way', 7, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['die', 7, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['pass', 8, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['law', 8, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['know', 8, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['past', 8, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['way', 8, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['die', 8, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['base', 8, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['see', 9, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['law', 9, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['know', 9, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['way', 9, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['die', 9, 4, 'doc2vec', 3, 0.7234101295471191, 5.529366864829698],\n",
       " ['earl', 1, 4, 'doc2vec', 4, 0.7207200527191162, 5.550005144034624],\n",
       " ['earl', 2, 4, 'doc2vec', 4, 0.7207200527191162, 5.550005144034624],\n",
       " ['earl', 3, 4, 'doc2vec', 4, 0.7207200527191162, 5.550005144034624],\n",
       " ['arm', 4, 4, 'doc2vec', 4, 0.7207200527191162, 5.550005144034624],\n",
       " ['give', 4, 4, 'doc2vec', 4, 0.7207200527191162, 5.550005144034624],\n",
       " ['peer', 4, 4, 'doc2vec', 4, 0.7207200527191162, 5.550005144034624],\n",
       " ['earl', 5, 4, 'doc2vec', 4, 0.7207200527191162, 5.550005144034624],\n",
       " ['coat', 6, 4, 'doc2vec', 4, 0.7207200527191162, 5.550005144034624],\n",
       " ['house', 6, 4, 'doc2vec', 4, 0.7207200527191162, 5.550005144034624],\n",
       " ['coat', 7, 4, 'doc2vec', 4, 0.7207200527191162, 5.550005144034624],\n",
       " ['duke', 7, 4, 'doc2vec', 4, 0.7207200527191162, 5.550005144034624],\n",
       " ['earl', 7, 4, 'doc2vec', 4, 0.7207200527191162, 5.550005144034624],\n",
       " ['house', 8, 4, 'doc2vec', 4, 0.7207200527191162, 5.550005144034624],\n",
       " ['earl', 8, 4, 'doc2vec', 4, 0.7207200527191162, 5.550005144034624],\n",
       " ['duke', 9, 4, 'doc2vec', 4, 0.7207200527191162, 5.550005144034624],\n",
       " ['earl', 9, 4, 'doc2vec', 4, 0.7207200527191162, 5.550005144034624],\n",
       " ['peer', 10, 4, 'doc2vec', 4, 0.7207200527191162, 5.550005144034624],\n",
       " ['white', 2, 3, 'topicModel', 1, 0.18716377, 16.02874315265787],\n",
       " ['marie', 5, 3, 'topicModel', 1, 0.18716377, 16.02874315265787],\n",
       " ['marie', 8, 3, 'topicModel', 1, 0.18716377, 16.02874315265787],\n",
       " ['louis', 1, 4, 'topicModel', 1, 0.18716377, 21.371657536877162],\n",
       " ['marie', 3, 4, 'topicModel', 1, 0.18716377, 21.371657536877162],\n",
       " ['white', 5, 4, 'topicModel', 1, 0.18716377, 21.371657536877162],\n",
       " ['white', 7, 4, 'topicModel', 1, 0.18716377, 21.371657536877162],\n",
       " ['mary', 10, 4, 'topicModel', 1, 0.18716377, 21.371657536877162]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_substitutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['prize', 'doc2vec', 1, 0.7421815395355225],\n",
       " ['use', 'doc2vec', 1, 0.7421815395355225],\n",
       " ['monarchy', 'doc2vec', 1, 0.7421815395355225],\n",
       " ['head', 'doc2vec', 1, 0.7421815395355225],\n",
       " ['chain', 'doc2vec', 1, 0.7421815395355225],\n",
       " ['choose', 'doc2vec', 1, 0.7421815395355225],\n",
       " ['receive', 'doc2vec', 1, 0.7421815395355225],\n",
       " ['crown', 'doc2vec', 1, 0.7421815395355225],\n",
       " ['queen', 'doc2vec', 1, 0.7421815395355225],\n",
       " ['win', 'doc2vec', 1, 0.7421815395355225],\n",
       " ['example', 'doc2vec', 1, 0.7421815395355225],\n",
       " ['couple', 'doc2vec', 1, 0.7421815395355225],\n",
       " ['monarch', 'doc2vec', 1, 0.7421815395355225],\n",
       " ['girl', 'doc2vec', 1, 0.7421815395355225],\n",
       " ['winners', 'doc2vec', 1, 0.7421815395355225],\n",
       " ['flower', 'doc2vec', 1, 0.7421815395355225],\n",
       " ['dance', 'doc2vec', 1, 0.7421815395355225],\n",
       " ['contest', 'doc2vec', 1, 0.7421815395355225],\n",
       " ['pageant', 'doc2vec', 1, 0.7421815395355225],\n",
       " ['occasion', 'doc2vec', 1, 0.7421815395355225],\n",
       " ['call', 'doc2vec', 1, 0.7421815395355225],\n",
       " ['headdress', 'doc2vec', 1, 0.7421815395355225],\n",
       " ['nothing', 'doc2vec', 1, 0.7421815395355225],\n",
       " ['symbol', 'doc2vec', 1, 0.7421815395355225],\n",
       " ['king', 'doc2vec', 1, 0.7421815395355225],\n",
       " ['wear', 'doc2vec', 1, 0.7421815395355225],\n",
       " ['part', 'doc2vec', 1, 0.7421815395355225],\n",
       " ['award', 'doc2vec', 1, 0.7421815395355225],\n",
       " ['type', 'doc2vec', 1, 0.7421815395355225],\n",
       " ['daisy', 'doc2vec', 1, 0.7421815395355225],\n",
       " ['beauty', 'doc2vec', 1, 0.7421815395355225],\n",
       " ['children', 'doc2vec', 1, 0.7421815395355225],\n",
       " ['chivalry', 'doc2vec', 2, 0.7250586152076721],\n",
       " ['title', 'doc2vec', 2, 0.7250586152076721],\n",
       " ['rank', 'doc2vec', 2, 0.7250586152076721],\n",
       " ['members', 'doc2vec', 2, 0.7250586152076721],\n",
       " ['baronets', 'doc2vec', 2, 0.7250586152076721],\n",
       " ['sir', 'doc2vec', 2, 0.7250586152076721],\n",
       " ['barons', 'doc2vec', 2, 0.7250586152076721],\n",
       " ['kingdom', 'doc2vec', 2, 0.7250586152076721],\n",
       " ['baronet', 'doc2vec', 2, 0.7250586152076721],\n",
       " ['order', 'doc2vec', 2, 0.7250586152076721],\n",
       " ['bachelor', 'doc2vec', 2, 0.7250586152076721],\n",
       " ['man', 'doc2vec', 2, 0.7250586152076721],\n",
       " ['pass', 'doc2vec', 2, 0.7250586152076721],\n",
       " ['children', 'doc2vec', 2, 0.7250586152076721],\n",
       " ['knight', 'doc2vec', 2, 0.7250586152076721],\n",
       " ['sister', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['primogeniture', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['inheritance', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['monarchy', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['see', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['pass', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['child', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['cousin', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['throne', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['question', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['oldest', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['command', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['law', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['crown', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['quit', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['create', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['beforehand', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['monarch', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['family', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['time', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['monarchies', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['know', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['monarchs', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['idea', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['system', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['member', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['past', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['king', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['arrange', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['way', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['nowadays', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['methods', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['die', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['succession', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['brother', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['order', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['base', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['children', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['loyalty', 'doc2vec', 3, 0.7234101295471191],\n",
       " ['ceremonies', 'doc2vec', 4, 0.7207200527191162],\n",
       " ['college', 'doc2vec', 4, 0.7207200527191162],\n",
       " ['charge', 'doc2vec', 4, 0.7207200527191162],\n",
       " ['lord', 'doc2vec', 4, 0.7207200527191162],\n",
       " ['arm', 'doc2vec', 4, 0.7207200527191162],\n",
       " ['household', 'doc2vec', 4, 0.7207200527191162],\n",
       " ['give', 'doc2vec', 4, 0.7207200527191162],\n",
       " ['coat', 'doc2vec', 4, 0.7207200527191162],\n",
       " ['house', 'doc2vec', 4, 0.7207200527191162],\n",
       " ['marshal', 'doc2vec', 4, 0.7207200527191162],\n",
       " ['duke', 'doc2vec', 4, 0.7207200527191162],\n",
       " ['hereditary', 'doc2vec', 4, 0.7207200527191162],\n",
       " ['peer', 'doc2vec', 4, 0.7207200527191162],\n",
       " ['earl', 'doc2vec', 4, 0.7207200527191162],\n",
       " ['permission', 'doc2vec', 4, 0.7207200527191162],\n",
       " ['elect', 'doc2vec', 4, 0.7207200527191162],\n",
       " ['issue', 'doc2vec', 4, 0.7207200527191162],\n",
       " ['royal', 'doc2vec', 4, 0.7207200527191162],\n",
       " ['member', 'doc2vec', 4, 0.7207200527191162],\n",
       " ['processions', 'doc2vec', 4, 0.7207200527191162],\n",
       " ['white', 'topicModel', 1, 0.18716377],\n",
       " ['marie', 'topicModel', 1, 0.18716377],\n",
       " ['marie', 'topicModel', 1, 0.18716377],\n",
       " ['louis', 'topicModel', 1, 0.18716377],\n",
       " ['marie', 'topicModel', 1, 0.18716377],\n",
       " ['white', 'topicModel', 1, 0.18716377],\n",
       " ['white', 'topicModel', 1, 0.18716377],\n",
       " ['mary', 'topicModel', 1, 0.18716377]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_words_considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_topic_words_to_print(topic_words):\n",
    "    output = [[]]\n",
    "    topic_number = 1\n",
    "    for topic in topic_words:\n",
    "        if topic[1] == 'topicModel':\n",
    "            return output\n",
    "        if topic[2] != topic_number:\n",
    "            topic_number = topic[2]\n",
    "            output.append([topic[0]])\n",
    "        output[topic_number-1].append(topic[0])\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['prize',\n",
       "  'use',\n",
       "  'monarchy',\n",
       "  'head',\n",
       "  'chain',\n",
       "  'choose',\n",
       "  'receive',\n",
       "  'crown',\n",
       "  'queen',\n",
       "  'win',\n",
       "  'example',\n",
       "  'couple',\n",
       "  'monarch',\n",
       "  'girl',\n",
       "  'winners',\n",
       "  'flower',\n",
       "  'dance',\n",
       "  'contest',\n",
       "  'pageant',\n",
       "  'occasion',\n",
       "  'call',\n",
       "  'headdress',\n",
       "  'nothing',\n",
       "  'symbol',\n",
       "  'king',\n",
       "  'wear',\n",
       "  'part',\n",
       "  'award',\n",
       "  'type',\n",
       "  'daisy',\n",
       "  'beauty',\n",
       "  'children'],\n",
       " ['chivalry',\n",
       "  'chivalry',\n",
       "  'title',\n",
       "  'rank',\n",
       "  'members',\n",
       "  'baronets',\n",
       "  'sir',\n",
       "  'barons',\n",
       "  'kingdom',\n",
       "  'baronet',\n",
       "  'order',\n",
       "  'bachelor',\n",
       "  'man',\n",
       "  'pass',\n",
       "  'children',\n",
       "  'knight'],\n",
       " ['sister',\n",
       "  'sister',\n",
       "  'primogeniture',\n",
       "  'inheritance',\n",
       "  'monarchy',\n",
       "  'see',\n",
       "  'pass',\n",
       "  'child',\n",
       "  'cousin',\n",
       "  'throne',\n",
       "  'question',\n",
       "  'oldest',\n",
       "  'command',\n",
       "  'law',\n",
       "  'crown',\n",
       "  'quit',\n",
       "  'create',\n",
       "  'beforehand',\n",
       "  'monarch',\n",
       "  'family',\n",
       "  'time',\n",
       "  'monarchies',\n",
       "  'know',\n",
       "  'monarchs',\n",
       "  'idea',\n",
       "  'system',\n",
       "  'member',\n",
       "  'past',\n",
       "  'king',\n",
       "  'arrange',\n",
       "  'way',\n",
       "  'nowadays',\n",
       "  'methods',\n",
       "  'die',\n",
       "  'succession',\n",
       "  'brother',\n",
       "  'order',\n",
       "  'base',\n",
       "  'children',\n",
       "  'loyalty'],\n",
       " ['ceremonies',\n",
       "  'ceremonies',\n",
       "  'college',\n",
       "  'charge',\n",
       "  'lord',\n",
       "  'arm',\n",
       "  'household',\n",
       "  'give',\n",
       "  'coat',\n",
       "  'house',\n",
       "  'marshal',\n",
       "  'duke',\n",
       "  'hereditary',\n",
       "  'peer',\n",
       "  'earl',\n",
       "  'permission',\n",
       "  'elect',\n",
       "  'issue',\n",
       "  'royal',\n",
       "  'member',\n",
       "  'processions']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_topic_words_to_print(topic_words_considered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [pun.tokenize(thing, stem=False, initial_word_split=False) for thing in wiki_doc2vec_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list_of_words = [word[0] for word in topic_words_considered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('choose', 'NN')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pun.pos_tag(['choose'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. right topic \n",
    "2. Does it sound similar \n",
    "\n",
    "implement live validation *********\n",
    "\n",
    "Conversational corpus \n",
    "\n",
    "multi-arm banding \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
